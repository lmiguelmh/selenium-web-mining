{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bitcoin news corpus\n",
    "- English news about \"bitcoin\" since Oct 01 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories: ['20161001', '20161002', '20161003', '20161004', '20161005'] ...\nretrieving \"data\\all_words.pkz\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving \"data\\all_lemmatized_words.pkz\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity for word \"bitcoin\"\nin original words:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blockchain cryptocurrency bitcoins technology ethereum news digital\nexchange new use price financial transactions based network zcash\nmarket online currency china\nin lemmatized words:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blockchain cryptocurrency technology exchange news bitcoins digital\nnew ethereum payment transaction price market use financial user like\nnetwork currency mining\nsimilarity for word \"china\"\nin original words:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitcoin chinese blockchain price india u exchanges adoption technology\nexchange network cryptocurrency mining community company japan news\nkorea first trading\nin lemmatized words:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitcoin chinese blockchain u price exchange india technology company\nnetwork adoption wallet mining startup country ecosystem one currency\ncryptocurrency community\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "\n",
    "def save_or_retrieve(file_pkz, func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    will save or retrieve the contents returned by func\n",
    "    :param file_pkz: path\n",
    "    :param func: function or lambda for easy use\n",
    "    :param args: args for func\n",
    "    :param kwargs: args for func\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_pkz):\n",
    "        print('saving \"%s\"' % file_pkz)\n",
    "        content = func(*args, **kwargs)\n",
    "        compressed_content = codecs.encode(pickle.dumps(content), 'zlib_codec')\n",
    "        with open(file_pkz, 'wb') as f:\n",
    "            f.write(compressed_content)\n",
    "        return content\n",
    "    else:\n",
    "        print('retrieving \"%s\"' % file_pkz)\n",
    "        with open(file_pkz, 'rb') as f:\n",
    "            compressed_content = f.read()\n",
    "        return pickle.loads(codecs.decode(compressed_content, 'zlib_codec'))\n",
    "\n",
    "corpus_dir = 'links_contents'\n",
    "working_dir = 'data'\n",
    "\n",
    "english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "corpus_reader = nltk.corpus.CategorizedPlaintextCorpusReader(corpus_dir, r'.*.txt', cat_pattern=r'([0-9]+)-.*')\n",
    "categories = corpus_reader.categories()\n",
    "print('categories:', categories[:5], '...')\n",
    "\n",
    "all_words_pkz = os.path.join(working_dir, 'all_words.pkz')\n",
    "all_words = save_or_retrieve(\n",
    "    all_words_pkz,\n",
    "    lambda: nltk.Text(w.lower() for w in corpus_reader.words() if w.lower() not in english_stopwords))\n",
    "\n",
    "all_lemmatized_words_pkz = os.path.join(working_dir, 'all_lemmatized_words.pkz')\n",
    "all_lemmatized_words = save_or_retrieve(\n",
    "    all_lemmatized_words_pkz,\n",
    "    lambda: nltk.Text(wnl.lemmatize(w.lower()) for w in corpus_reader.words() if w.lower() not in english_stopwords))\n",
    "\n",
    "words = ['bitcoin', 'china']  # lowercase\n",
    "for word in words:\n",
    "    # print('CONCORDANCE: ', word.upper())\n",
    "    # all_text.concordance(word, width=50)\n",
    "    # print()\n",
    "    # Distributional similarity: find other words which appear in the same contexts as the specified word; list most similar words first.\n",
    "    print('similarity for word \"%s\"' % word)\n",
    "    print('in original words:')\n",
    "    all_words.similar(word)\n",
    "    print('in lemmatized words:')\n",
    "    all_lemmatized_words.similar(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning corpus\n",
    "- Deleting lines with less than 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_lines(lines, preprocess=True):\n",
    "    if not preprocess:\n",
    "        return lines\n",
    "    else:\n",
    "        preprocessed_lines = []\n",
    "        for line in lines:\n",
    "            # TODO a NN to recognize the type of word? NUMBER, PERCENT, HOUR, DATE, ABBR, (y hellooo!)\n",
    "\n",
    "            # standardize perc values  --  remember ?: non capturing group\n",
    "            line = re.sub(r'[+\\-]?(?:\\d*[.,]?\\d+|\\d+[.,]?\\d*)%', 'PERCENTAGE', line)\n",
    "\n",
    "            # standardize monetary values $1.5M, $1., $.3\n",
    "            line = re.sub(r'\\$\\d*(?:\\d*[.,]?\\d+|\\d+[.,]?\\d*)[Mm]?', 'MONEY', line)\n",
    "\n",
    "            # standardize certain decimal values like \"2.0\" or \".2\" not \"2016.\"\n",
    "            line = re.sub('\\d*[.,]\\d+', 'DECIMAL', line)\n",
    "\n",
    "            # standardize num values - not by now too many false positives\n",
    "            # line = re.sub('\\d*(?:\\d*[.,]?\\d+|\\d+[.,]?\\d*)', 'NUMBER', line)\n",
    "\n",
    "            # standardize abbreviations i.e. -> ie, u.s.a. -> usa, but not ee.uu.\n",
    "            # there is no way to multiple re group captures http://stackoverflow.com/a/464879/2692914\n",
    "            p = re.compile(r'\\b((?:[a-zA-Z]\\.)+)')\n",
    "            for m in p.finditer(line):\n",
    "                line = line.replace(m.group(), ''.join(m.group(1).split('.')))\n",
    "\n",
    "            # separate in sentences\n",
    "            # sentences = re.split(r'(?:.\\s+)', line)\n",
    "            sentences = [s for s in re.split(r'\\.+', line) if len(s.split()) > 2]\n",
    "            for s in sentences:\n",
    "                preprocessed_lines.append(s)\n",
    "        return preprocessed_lines\n",
    "\n",
    "\n",
    "def clean_file(in_file, out_file, preprocess=True):\n",
    "    # min words per paragraph\n",
    "    min_words_per_paragraph = 7\n",
    "    with open(in_file, 'r', encoding='utf8') as f:\n",
    "        lines = [line.rstrip('\\n') for line in f if len(line.split()) > min_words_per_paragraph]\n",
    "\n",
    "    # cleaning and preprocessing files\n",
    "    if preprocess:\n",
    "        lines = clean_lines(lines)\n",
    "        \n",
    "    # QUITAR ESPACIOS DE LAS LINEAS! \n",
    "    # USAR EL SERVIDOR DEDICADO CON UN POS TAGGER! http://stanfordnlp.github.io/CoreNLP/corenlp-server.html#dedicated-server \n",
    "    \n",
    "    content = '\\n'.join(lines)\n",
    "    # doenst work as expected, for semantic analysis we need some stopwords (i.e. have)\n",
    "    # content = content.lower()\n",
    "    # english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "    # wnl = nltk.WordNetLemmatizer()\n",
    "    # words = list(set(re.split(r'[^\\w]+', content)))\n",
    "    # for word in words:\n",
    "    #     if word in english_stopwords:\n",
    "    #         # ignore english stopwords\n",
    "    #         new_word = ''\n",
    "    #     else:\n",
    "    #         # lemmatize words\n",
    "    #         new_word = wnl.lemmatize(word)\n",
    "    #     content = re.sub(r'\\b%s\\b' % word, new_word, content)\n",
    "\n",
    "    with open(out_file, 'wb') as f:\n",
    "        f.write(bytes(content, encoding='utf8'))\n",
    "\n",
    "in_dir = 'links_contents'\n",
    "out_dir = 'links_contents_clean'\n",
    "files = [\n",
    "    '20161001-2fc38645b2f1075b-Community_Prefers_Bitcoin_As_Exchange_Medium_Over_Store_of_Value%2C.txt',\n",
    "    '20161001-51c397b0cd8bdbc6-Why_Blockchain_Won%E2%80%99t_Disrupt_Banks_First_-_CoinDesk.txt',\n",
    "    '20161001-adb0edbb66d3fb0d-Kim_Dotcom_Reiterates_His_Bitcoin_Price_Forecast%2C_%242000_in_2.txt',\n",
    "    '20161002-2c276484c2156687-Bitcoin_Can_Buy_You_a_Biometric_Data_Skimmer_on_the.txt',\n",
    "    '20161002-c2b624fa4e45d409-Croatian_Law_Enforcement_Completes_Another_Bitcoin-related_Darknet_Drug_Bust_-.txt',\n",
    "    '20161003-1eca38f66bf45f56-MGT_Capital_Investments_Inc_%28NYSEMKT%3AMGT%29%3A_Opportunity_Through_Uncertainty_%7C_Insider.txt'\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    in_file = os.path.join(in_dir, file)\n",
    "    # out_file = os.path.join(out_dir, file)\n",
    "    # clean_file(in_file, out_file, False)\n",
    "    out_file = os.path.join(out_dir, file + \".clean.txt\")\n",
    "    clean_file(in_file, out_file, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Stanford's CoreNLP for sentiment analysis and entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\nSentiment: Neutral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Neutral\nSentiment: Negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\nSentiment: Negative\nSentiment: Negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\nSentiment: Neutral\nSentiment: Negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\nSentiment: Negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\nSentiment: Positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\nSentiment: Neutral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\nSentiment: Negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\nSentiment: Negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive\nSentiment: Negative\nSentiment: Negative\nSentiment: Neutral\nSentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "# http://stanfordnlp.github.io/CoreNLP/corenlp-server.html#getting-started\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "def corenlp_analysis_all(in_file):\n",
    "    with open(in_file, 'r', encoding='utf8') as f:\n",
    "        content = \"\".join(f.readlines())\n",
    "\n",
    "    nlp = StanfordCoreNLP('http://127.0.0.1:9000')\n",
    "    output = nlp.annotate(\n",
    "        content,\n",
    "        properties={\n",
    "            'annotators': 'sentiment',\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for i, _ in enumerate(output['sentences']):\n",
    "        print(\"Sentence:\", [t['word'] for t in output['sentences'][i]['tokens']])\n",
    "        print(\"Sentiment:\", output['sentences'][i]['sentiment'])\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "def corenlp_analysis(in_file):\n",
    "    with open(in_file, 'r', encoding='utf8') as f:\n",
    "        # content = f.readlines()\n",
    "        lines = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "    nlp = StanfordCoreNLP('http://127.0.0.1:9000')\n",
    "    for line in lines:\n",
    "        output = nlp.annotate(\n",
    "            line,\n",
    "            properties={\n",
    "                'annotators': 'sentiment,ner,pos',\n",
    "                'outputFormat': 'json'\n",
    "            }\n",
    "        )\n",
    "        # http://stanfordnlp.github.io/CoreNLP/ner.html\n",
    "        ner_names = [\"PERSON\", \"LOCATION\", \"ORGANIZATION\", \"MISC\"]\n",
    "        ner_numerical = [\"MONEY\", \"NUMBER\", \"ORDINAL\", \"PERCENT\"]\n",
    "        ner_temporal = [\"DATE\", \"TIME\", \"DURATION\", \"SET\"]\n",
    "        for i, _ in enumerate(output['sentences']):\n",
    "            # print([t['word'] + (\"/\" + t[\"pos\"]) + (\":\" + t[\"ner\"] if t[\"ner\"] != \"O\" else \"\") for t in output['sentences'][i]['tokens']])\n",
    "            print(\"Sentiment:\", output['sentences'][i]['sentiment'])\n",
    "            # print(\"\")\n",
    "\n",
    "\n",
    "in_dir = 'links_contents_clean'\n",
    "files = [\n",
    "    '20161001-2fc38645b2f1075b-Community_Prefers_Bitcoin_As_Exchange_Medium_Over_Store_of_Value%2C.txt.clean.txt',\n",
    "    # '20161001-51c397b0cd8bdbc6-Why_Blockchain_Won%E2%80%99t_Disrupt_Banks_First_-_CoinDesk.txt.clean.txt',\n",
    "    # '20161001-adb0edbb66d3fb0d-Kim_Dotcom_Reiterates_His_Bitcoin_Price_Forecast%2C_%242000_in_2.txt.clean.txt',\n",
    "    # '20161002-2c276484c2156687-Bitcoin_Can_Buy_You_a_Biometric_Data_Skimmer_on_the.txt.clean.txt',\n",
    "    # '20161002-c2b624fa4e45d409-Croatian_Law_Enforcement_Completes_Another_Bitcoin-related_Darknet_Drug_Bust_-.txt.clean.txt',\n",
    "    # '20161003-1eca38f66bf45f56-MGT_Capital_Investments_Inc_%28NYSEMKT%3AMGT%29%3A_Opportunity_Through_Uncertainty_%7C_Insider.tx.clean.txtt'\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    in_file = os.path.join(in_dir, file)\n",
    "    corenlp_analysis(in_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problema: los títulos están en mayúsculas, debemos reconocer las entidades para ello ignoramos sólo tomaremos en cuenta oraciones que contengan como mínimo un 10% de palabras en minúscula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'J. Christina Wang': 'PERSON', 'Federal Reserve Bank of Boston': 'ORGANIZATION', '41 %': 'PERCENT', 'Foundation': 'ORGANIZATION', '37 %': 'PERCENT', '18 %': 'PERCENT', 'currently': 'DATE', 'one': 'NUMBER', '36 %': 'PERCENT', '11 %': 'PERCENT', 'Stephanie Lo': 'PERSON', '12 %': 'PERCENT', '20 %': 'PERCENT', '27 %': 'PERCENT', 'end 2017': 'DATE', '28 %': 'PERCENT', 'Harvard University': 'ORGANIZATION', 'Facebook': 'ORGANIZATION', 'Matthew R. Silver of Pepper Hamilton LLP': 'ORGANIZATION', 'Twitter': 'ORGANIZATION', 'Bitcoin': 'PERSON', 'Board': 'ORGANIZATION', 'the next 12 months': 'DURATION', '19 %': 'PERCENT', 'Timothy R. McTaggart': 'PERSON', '15 %': 'PERCENT', 'Llew Claasen': 'PERSON', '62 %': 'PERCENT', 'the past': 'DATE', '35 %': 'PERCENT', 'Claasen': 'PERSON', 'Cointelegraph': 'PERSON', '14 %': 'PERCENT', 'Bitcoin Foundation': 'ORGANIZATION', '42 %': 'PERCENT', 'The Foundation': 'ORGANIZATION', 'quarterly': 'SET', '2016/2017': 'NUMBER', 'Reddit': 'LOCATION', '10 %': 'PERCENT'}\n"
     ]
    }
   ],
   "source": [
    "# para poner en proper-case\n",
    "# - reconocimiento de entidades\n",
    "# - y sólo los nnp pueden serlo\n",
    "#\n",
    "# el número de menciones de la palabra según \"coreference\"\n",
    "# - \"coreference\" only works when words are in capitalized and not capitalized\n",
    "#\n",
    "# el sentimiento positivo, negativo, neutral\n",
    "# - de sólo el título\n",
    "# - de sólo las sentencias con menciones\n",
    "\n",
    "\n",
    "def retrieve_named_entities(lines, min_lower_words_per_sentence_rate=0.10):\n",
    "    nlp = StanfordCoreNLP('http://127.0.0.1:9000')\n",
    "    entity_words = {}\n",
    "    \n",
    "    # ignore the lines with more than 90% of uppercase\n",
    "    content = \"\"\n",
    "    for line in lines:\n",
    "        lower_words = 0\n",
    "        words = line.split()\n",
    "        # todo bad performance\n",
    "        for word in words:\n",
    "            if word[0].isalpha() and word[0].islower():\n",
    "                lower_words += 1\n",
    "        if lower_words > len(words) * min_lower_words_per_sentence_rate:\n",
    "            content += line + \"\\n\"\n",
    "\n",
    "    output = nlp.annotate(\n",
    "        content,\n",
    "        properties={\n",
    "            'annotators': 'ner',\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "    )\n",
    "    # http://stanfordnlp.github.io/CoreNLP/ner.html\n",
    "    entity = \"\"\n",
    "    entity_words = {}\n",
    "    for i, _ in enumerate(output['sentences']):\n",
    "        words = []\n",
    "        for t in output['sentences'][i]['tokens']:\n",
    "            if entity != t[\"ner\"]:\n",
    "                if len(words) > 0:\n",
    "                    entity_words[\" \".join(words)] = entity\n",
    "                    words = []\n",
    "\n",
    "            entity = t[\"ner\"]\n",
    "            if entity != 'O':\n",
    "                words.append(t[\"word\"])\n",
    "                # print([t[\"word\"] for t in output['sentences'][i]['tokens']])\n",
    "                # print(entity_words)\n",
    "    return entity_words\n",
    "\n",
    "\n",
    "in_dir = 'links_contents_clean'\n",
    "files = [\n",
    "    '20161001-2fc38645b2f1075b-Community_Prefers_Bitcoin_As_Exchange_Medium_Over_Store_of_Value%2C.txt.clean.txt',\n",
    "    # '20161001-51c397b0cd8bdbc6-Why_Blockchain_Won%E2%80%99t_Disrupt_Banks_First_-_CoinDesk.txt.clean.txt',\n",
    "    # '20161001-adb0edbb66d3fb0d-Kim_Dotcom_Reiterates_His_Bitcoin_Price_Forecast%2C_%242000_in_2.txt.clean.txt',\n",
    "    # '20161002-2c276484c2156687-Bitcoin_Can_Buy_You_a_Biometric_Data_Skimmer_on_the.txt.clean.txt',\n",
    "    # '20161002-c2b624fa4e45d409-Croatian_Law_Enforcement_Completes_Another_Bitcoin-related_Darknet_Drug_Bust_-.txt.clean.txt',\n",
    "    # '20161003-1eca38f66bf45f56-MGT_Capital_Investments_Inc_%28NYSEMKT%3AMGT%29%3A_Opportunity_Through_Uncertainty_%7C_Insider.tx.clean.txtt'\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    in_file = os.path.join(in_dir, file)\n",
    "    with open(in_file, 'r', encoding='utf8') as f:\n",
    "        # content = f.readlines()\n",
    "        lines = [line.rstrip('\\n') for line in f]\n",
    "    print(retrieve_named_entities(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Negative': 27, 'Neutral': 3, 'Positive': 2})\n"
     ]
    }
   ],
   "source": [
    "# ** ojo podrìamos usar las named entities para encontrar el tf-idf like de cada uno!\n",
    "# \n",
    "# detectar fecha y hora de artìculo\n",
    "# \n",
    "# podemos buscar caracterìsticas\n",
    "# - nùmero de sentencias positivas\n",
    "# - nùmero de sentencias negativas\n",
    "# - nùmero de sentencias neutras\n",
    "# - nùmero de menciones de named entities \n",
    "# - tf-idf\n",
    "# \n",
    "# y con esto decimos si es positivo o negativo segùn el precio de la bolsa!!!\n",
    "# \n",
    "# \n",
    "# para poner en proper-case\n",
    "# - reconocimiento de entidades\n",
    "# - y sólo los nnp pueden serlo\n",
    "# \n",
    "# el número de menciones de la palabra según \"coreference\"\n",
    "# - \"coreference\" only works when words are in capitalized and not capitalized\n",
    "# \n",
    "# el sentimiento positivo, negativo, neutral\n",
    "# - de sólo el título\n",
    "# - de sólo las sentencias con menciones\n",
    "# \n",
    "from collections import Counter\n",
    "\n",
    "def analize_sentiment(content):\n",
    "    nlp = StanfordCoreNLP('http://127.0.0.1:9000')\n",
    "    output = nlp.annotate(\n",
    "        content,\n",
    "        properties={\n",
    "            'annotators': 'sentiment',\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "    )\n",
    "    # http://stanfordnlp.github.io/CoreNLP/sentiment.html#options\n",
    "    sentiment = [s['sentiment'] for s in output['sentences']]\n",
    "    return Counter(sentiment)\n",
    "    \n",
    "\n",
    "in_dir = 'links_contents_clean'\n",
    "files = [\n",
    "    '20161001-2fc38645b2f1075b-Community_Prefers_Bitcoin_As_Exchange_Medium_Over_Store_of_Value%2C.txt.clean.txt',\n",
    "    # '20161001-51c397b0cd8bdbc6-Why_Blockchain_Won%E2%80%99t_Disrupt_Banks_First_-_CoinDesk.txt.clean.txt',\n",
    "    # '20161001-adb0edbb66d3fb0d-Kim_Dotcom_Reiterates_His_Bitcoin_Price_Forecast%2C_%242000_in_2.txt.clean.txt',\n",
    "    # '20161002-2c276484c2156687-Bitcoin_Can_Buy_You_a_Biometric_Data_Skimmer_on_the.txt.clean.txt',\n",
    "    # '20161002-c2b624fa4e45d409-Croatian_Law_Enforcement_Completes_Another_Bitcoin-related_Darknet_Drug_Bust_-.txt.clean.txt',\n",
    "    # '20161003-1eca38f66bf45f56-MGT_Capital_Investments_Inc_%28NYSEMKT%3AMGT%29%3A_Opportunity_Through_Uncertainty_%7C_Insider.tx.clean.txtt'\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    in_file = os.path.join(in_dir, file)\n",
    "    with open(in_file, 'r', encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "    sentiment = analize_sentiment(\"\\n\".join(lines))\n",
    "    print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 a1b2c3d4\n10 sd\n14 sd\n17 abc\n\"---w---w---w...w,w-w*w/w\"\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "# # p = re.compile(r'\\\\b+.*\\\\b+')\n",
    "p = re.compile(r'(?:\\w+\\b|\\b\\w+\\b|\\b\\w+)')\n",
    "for m in p.finditer(' a1b2c3d4 sd. sd;abc'):\n",
    "    print(m.start(), m.group())\n",
    "s = re.sub(r'\\b\\w+\\b', 'w', '---aaa---word---word...word,wprd-w*2016/2017')\n",
    "print('\\\"%s\\\"' % s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'bc', 'de', 'aa', 'aaa', 'word', 'word', 'word', 'wprd', 'w', '2016', '2017']\n"
     ]
    }
   ],
   "source": [
    "words = re.split(r'[^\\w]+', 'aa bc de--aa ---aaa---word---word...word,wprd-w*2016/2017')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('have' in english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usa ee.uu. write ie Timothy R McTaggart and Matthew R Silver of. my example DECIMAL MegaUpload DECIMAL and BitCache 1. Read more 2016...\n"
     ]
    }
   ],
   "source": [
    "line = \"u.s.a. ee.uu. write i.e. Timothy R. McTaggart and Matthew R. Silver of. my example .1 MegaUpload 2.0 and BitCache 1. Read more 2016...\"\n",
    "line = re.sub('\\d*[.,]\\d+', 'DECIMAL', line)\n",
    "\n",
    "# print(re.sub(r'\\b(([a-zA-Z])\\.)+', '\\g<2>', line))\n",
    "# print(re.sub(r'\\b(?:([a-zA-Z])\\.)+', '*', line))\n",
    "p = re.compile(r'\\b((?:[a-zA-Z]\\.)+)')\n",
    "for m in p.finditer(line):\n",
    "    line = line.replace(m.group(), ''.join(m.group(1).split('.')))\n",
    "    # print(m.start(), m.group(), ''.join(m.group(1).split('.')))\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it usa ee', 'uu', ' write ie Timothy R McTaggart and Matthew', ' Inc (NYSEMKT:MGT)', ' my example MegaUpload 2', '0 and BitCache', ' Read more', ' \"hello worl', '\"']\n['it usa ee', ' write ie Timothy R McTaggart and Matthew', ' my example MegaUpload 2', '0 and BitCache']\n"
     ]
    }
   ],
   "source": [
    "line = 'it usa ee.uu. write ie Timothy R McTaggart and Matthew. Inc (NYSEMKT:MGT). my example MegaUpload 2.0 and BitCache. Read more... \"hello worl.\"'\n",
    "#FAILS WITH: MGT Capital intends to change its corporate name to “John McAfee Global Technologies, Inc.”\n",
    "print([s for s in re.split(r'\\.', line) if s])\n",
    "print([s for s in re.split(r'\\.+', line) if len(s.split()) > 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many in the Bitcoin community have expressed that the most compelling function they want the Bitcoin Foundation to focus its advocacy programs on in the next 12 months is; serving as a medium of exchange.\nAccording to the outcomes of a community survey released by the Foundation, 37% of the respondents choose to focus on Bitcoin as a medium of exchange - i.e. used to\n\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "\"Many in the Bitcoin community have expressed that the most compelling function they want the Bitcoin Foundation to focus its advocacy programs on in the next 12 months is; serving as a medium of exchange.\",\n",
    "\"Community Prefers Bitcoin As Exchange Medium Over Store of Value, Survey Shows\", \n",
    "\"Community Prefers Bitcoin As Exchange Medium Over Store of Value, Survey Shows\", \n",
    "\"According to the outcomes of a community survey released by the Foundation, 37% of the respondents choose to focus on Bitcoin as a medium of exchange - i.e. used to\", \n",
    "]\n",
    "content = \"\"\n",
    "for line in lines:\n",
    "    lower_words = 0\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        if word[0].isalpha() and word[0].islower():\n",
    "            lower_words += 1\n",
    "    if lower_words > len(words) * 0.10:\n",
    "        content += line + \"\\n\"\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}