{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bitcoin news corpus\n",
    "- English news about \"bitcoin\" since Oct 01 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories: ['20161001', '20161002', '20161003', '20161004', '20161005'] ...\nretrieving \"data\\all_words.pkz\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving \"data\\all_lemmatized_words.pkz\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity for word \"bitcoin\"\nin original words:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blockchain cryptocurrency bitcoins technology ethereum news digital\nexchange new use price financial transactions based network zcash\nmarket online currency china\nin lemmatized words:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blockchain cryptocurrency technology exchange news bitcoins digital\nnew ethereum payment transaction price market use financial user like\nnetwork currency mining\nsimilarity for word \"china\"\nin original words:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitcoin chinese blockchain price india u exchanges adoption technology\nexchange network cryptocurrency mining community company japan news\nkorea first trading\nin lemmatized words:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitcoin chinese blockchain u price exchange india technology company\nnetwork adoption wallet mining startup country ecosystem one currency\ncryptocurrency community\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "\n",
    "def save_or_retrieve(file_pkz, func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    will save or retrieve the contents returned by func\n",
    "    :param file_pkz: path\n",
    "    :param func: function or lambda for easy use\n",
    "    :param args: args for func\n",
    "    :param kwargs: args for func\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_pkz):\n",
    "        print('saving \"%s\"' % file_pkz)\n",
    "        content = func(*args, **kwargs)\n",
    "        compressed_content = codecs.encode(pickle.dumps(content), 'zlib_codec')\n",
    "        with open(file_pkz, 'wb') as f:\n",
    "            f.write(compressed_content)\n",
    "        return content\n",
    "    else:\n",
    "        print('retrieving \"%s\"' % file_pkz)\n",
    "        with open(file_pkz, 'rb') as f:\n",
    "            compressed_content = f.read()\n",
    "        return pickle.loads(codecs.decode(compressed_content, 'zlib_codec'))\n",
    "\n",
    "corpus_dir = 'links_contents'\n",
    "working_dir = 'data'\n",
    "\n",
    "english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "corpus_reader = nltk.corpus.CategorizedPlaintextCorpusReader(corpus_dir, r'.*.txt', cat_pattern=r'([0-9]+)-.*')\n",
    "categories = corpus_reader.categories()\n",
    "print('categories:', categories[:5], '...')\n",
    "\n",
    "all_words_pkz = os.path.join(working_dir, 'all_words.pkz')\n",
    "all_words = save_or_retrieve(\n",
    "    all_words_pkz,\n",
    "    lambda: nltk.Text(w.lower() for w in corpus_reader.words() if w.lower() not in english_stopwords))\n",
    "\n",
    "all_lemmatized_words_pkz = os.path.join(working_dir, 'all_lemmatized_words.pkz')\n",
    "all_lemmatized_words = save_or_retrieve(\n",
    "    all_lemmatized_words_pkz,\n",
    "    lambda: nltk.Text(wnl.lemmatize(w.lower()) for w in corpus_reader.words() if w.lower() not in english_stopwords))\n",
    "\n",
    "words = ['bitcoin', 'china']  # lowercase\n",
    "for word in words:\n",
    "    # print('CONCORDANCE: ', word.upper())\n",
    "    # all_text.concordance(word, width=50)\n",
    "    # print()\n",
    "    # Distributional similarity: find other words which appear in the same contexts as the specified word; list most similar words first.\n",
    "    print('similarity for word \"%s\"' % word)\n",
    "    print('in original words:')\n",
    "    all_words.similar(word)\n",
    "    print('in lemmatized words:')\n",
    "    all_lemmatized_words.similar(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning corpus\n",
    "- Deleting lines with less than 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_lines(lines, preprocess=True):\n",
    "    if not preprocess:\n",
    "        return lines\n",
    "    else:\n",
    "        preprocessed_lines = []\n",
    "        for line in lines:\n",
    "            # TODO a NN to recognize the type of word? NUMBER, PERCENT, HOUR, DATE, ABBR, (y hellooo!)\n",
    "\n",
    "            # standardize perc values  --  remember ?: non capturing group\n",
    "            line = re.sub(r'[+\\-]?(?:\\d*[.,]?\\d+|\\d+[.,]?\\d*)%', 'PERCENTAGE', line)\n",
    "\n",
    "            # standardize monetary values $1.5M, $1., $.3\n",
    "            line = re.sub(r'\\$\\d*(?:\\d*[.,]?\\d+|\\d+[.,]?\\d*)[Mm]?', 'MONEY', line)\n",
    "\n",
    "            # standardize certain decimal values like \"2.0\" or \".2\" not \"2016.\"\n",
    "            line = re.sub('\\d*[.,]\\d+', 'DECIMAL', line)\n",
    "\n",
    "            # standardize num values - not by now too many false positives\n",
    "            # line = re.sub('\\d*(?:\\d*[.,]?\\d+|\\d+[.,]?\\d*)', 'NUMBER', line)\n",
    "\n",
    "            # standardize abbreviations i.e. -> ie, u.s.a. -> usa, but not ee.uu.\n",
    "            # there is no way to multiple re group captures http://stackoverflow.com/a/464879/2692914\n",
    "            p = re.compile(r'\\b((?:[a-zA-Z]\\.)+)')\n",
    "            for m in p.finditer(line):\n",
    "                line = line.replace(m.group(), ''.join(m.group(1).split('.')))\n",
    "\n",
    "            # separate in sentences\n",
    "            # sentences = re.split(r'(?:.\\s+)', line)\n",
    "            sentences = [s for s in re.split(r'\\.+', line) if len(s.split()) > 2]\n",
    "            for s in sentences:\n",
    "                preprocessed_lines.append(s)\n",
    "        return preprocessed_lines\n",
    "\n",
    "\n",
    "def clean_file(in_file, out_file, preprocess=True):\n",
    "    # min words per paragraph\n",
    "    min_words_per_paragraph = 7\n",
    "    with open(in_file, 'r', encoding='utf8') as f:\n",
    "        lines = [line.rstrip('\\n') for line in f if len(line.split()) > min_words_per_paragraph]\n",
    "\n",
    "    # cleaning and preprocessing files\n",
    "    if preprocess:\n",
    "        lines = clean_lines(lines)\n",
    "        \n",
    "    # QUITAR ESPACIOS DE LAS LINEAS! \n",
    "    # USAR EL SERVIDOR DEDICADO CON UN POS TAGGER! http://stanfordnlp.github.io/CoreNLP/corenlp-server.html#dedicated-server \n",
    "    \n",
    "    content = '\\n'.join(lines)\n",
    "    # doenst work as expected, for semantic analysis we need some stopwords (i.e. have)\n",
    "    # content = content.lower()\n",
    "    # english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "    # wnl = nltk.WordNetLemmatizer()\n",
    "    # words = list(set(re.split(r'[^\\w]+', content)))\n",
    "    # for word in words:\n",
    "    #     if word in english_stopwords:\n",
    "    #         # ignore english stopwords\n",
    "    #         new_word = ''\n",
    "    #     else:\n",
    "    #         # lemmatize words\n",
    "    #         new_word = wnl.lemmatize(word)\n",
    "    #     content = re.sub(r'\\b%s\\b' % word, new_word, content)\n",
    "\n",
    "    with open(out_file, 'wb') as f:\n",
    "        f.write(bytes(content, encoding='utf8'))\n",
    "\n",
    "in_dir = 'links_contents'\n",
    "out_dir = 'links_contents_clean'\n",
    "files = [\n",
    "    '20161001-2fc38645b2f1075b-Community_Prefers_Bitcoin_As_Exchange_Medium_Over_Store_of_Value%2C.txt',\n",
    "    '20161001-51c397b0cd8bdbc6-Why_Blockchain_Won%E2%80%99t_Disrupt_Banks_First_-_CoinDesk.txt',\n",
    "    '20161001-adb0edbb66d3fb0d-Kim_Dotcom_Reiterates_His_Bitcoin_Price_Forecast%2C_%242000_in_2.txt',\n",
    "    '20161002-2c276484c2156687-Bitcoin_Can_Buy_You_a_Biometric_Data_Skimmer_on_the.txt',\n",
    "    '20161002-c2b624fa4e45d409-Croatian_Law_Enforcement_Completes_Another_Bitcoin-related_Darknet_Drug_Bust_-.txt',\n",
    "    '20161003-1eca38f66bf45f56-MGT_Capital_Investments_Inc_%28NYSEMKT%3AMGT%29%3A_Opportunity_Through_Uncertainty_%7C_Insider.txt'\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    in_file = os.path.join(in_dir, file)\n",
    "    # out_file = os.path.join(out_dir, file)\n",
    "    # clean_file(in_file, out_file, False)\n",
    "    out_file = os.path.join(out_dir, file + \".clean.txt\")\n",
    "    clean_file(in_file, out_file, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Stanford's CoreNLP for sentiment analysis and entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Community', 'Prefers', 'Bitcoin', 'As', 'Exchange', 'Medium', 'Over', 'Store', 'of', 'Value', ',', 'Survey', 'Shows']\nSentiment: Negative\n\n['Community', 'Prefers', 'Bitcoin', 'As', 'Exchange', 'Medium', 'Over', 'Store', 'of', 'Value', ',', 'Survey', 'Shows']\nSentiment: Negative\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Many', 'in', 'the', 'Bitcoin:LOCATION', 'community', 'have', 'expressed', 'that', 'the', 'most', 'compelling', 'function', 'they', 'want', 'the', 'Bitcoin:ORGANIZATION', 'Foundation:ORGANIZATION', 'to', 'focus', 'its', 'advocacy', 'programs', 'on', 'in', 'the:DURATION', 'next:DURATION', '12:DURATION', 'months:DURATION', 'is', ';', 'serving', 'as', 'a', 'medium', 'of', 'exchange', '.']\nSentiment: Positive\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['According', 'to', 'the', 'outcomes', 'of', 'a', 'community', 'survey', 'released', 'by', 'the', 'Foundation:ORGANIZATION', ',', '37:PERCENT', '%:PERCENT', 'of', 'the', 'respondents', 'choose', 'to', 'focus', 'on', 'Bitcoin', 'as', 'a', 'medium', 'of', 'exchange', '-', 'i.e.', 'used', 'to', 'actually', 'buy', 'things', '-', 'over', 'its', 'use', 'as', 'a', 'store', 'of', 'value', '-LRB-', '18:PERCENT', '%:PERCENT', '-RRB-', ',', '10:PERCENT', '%:PERCENT', 'who', 'do', 'not', 'know', 'and', '36:PERCENT', '%:PERCENT', 'that', 'opted', 'for', 'other', 'uses', '.']\nSentiment: Negative\n\n['Commenting', 'on', 'the', 'survey', ',', 'the', 'Executive', 'Director', 'of', 'The', 'Bitcoin:ORGANIZATION', 'Foundation:ORGANIZATION', ',', 'Llew:PERSON', 'Claasen:PERSON', ',', 'told', 'Cointelegraph:PERSON', ':']\nSentiment: Negative\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'The:ORGANIZATION', 'Foundation:ORGANIZATION', 'needs', 'to', 're-establish', 'legitimacy', 'amongst', 'long-time', 'Bitcoin:LOCATION', 'community', 'members', 'because', 'of', 'things', 'that', 'may', 'or', 'may', 'not', 'have', 'happened', 'in', 'the:DATE', 'past:DATE', 'through', 'predecessors', '.']\nSentiment: Negative\n\n['``']\nSentiment: Neutral\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'used', 'as', 'a', 'medium', 'of', 'exchange', ',', 'rather', 'than', 'as', 'an', 'investment', 'vehicle', ',', 'Bitcoin:PERSON', 'shows', 'users', 'many', 'potential', 'benefits', 'as', 'it', 'can', 'not', 'be', 'created', 'at', 'will', 'and', 'its', 'supply', 'is', 'finite', ',', 'write', 'Timothy:PERSON', 'R.:PERSON', 'McTaggart:PERSON', 'and', 'Matthew:ORGANIZATION', 'R.:ORGANIZATION', 'Silver:ORGANIZATION', 'of:ORGANIZATION', 'Pepper:ORGANIZATION', 'Hamilton:ORGANIZATION', 'LLP:ORGANIZATION', '.']\nSentiment: Negative\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['However', ',', 'Harvard:ORGANIZATION', 'University:ORGANIZATION', \"'s\", 'Stephanie:PERSON', 'Lo:PERSON', 'and', 'J.:PERSON', 'Christina:PERSON', 'Wang:PERSON', 'of', 'the', 'Federal:ORGANIZATION', 'Reserve:ORGANIZATION', 'Bank:ORGANIZATION', 'of:ORGANIZATION', 'Boston:ORGANIZATION', 'in', 'Bitcoin:LOCATION', 'as', 'Money', '?']\nSentiment: Neutral\n\n[',', 'noted', 'that', 'Bitcoin:PERSON', 'must', 'be', 'accepted', 'as', 'payment', 'for', 'a', 'sufficiently', 'large', 'set', 'of', 'goods', 'or', 'services', ',', 'or', 'other', 'assets', 'to', 'serve', 'as', 'a', 'medium', 'of', 'exchange', '.']\nSentiment: Negative\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'A', 'user', 'is', 'willing', 'to', 'accept', 'a', 'fiat', 'money', 'as', 'payment', 'for', 'other', 'objects', 'of', 'value', 'only', 'if', 'she', 'is', 'confident', 'that', 'enough', 'others', 'will', 'be', 'willing', 'to', 'accept', 'it', 'in', 'turn', 'from', 'her', '.']\nSentiment: Negative\n\n['Unlike', 'the', 'regular', 'fiat', 'money', ',', 'however', ',', 'Bitcoin:PERSON', 'is', 'not', 'backed', 'by', 'any', 'sovereign', 'entity', 'that', 'can', 'compel', 'the', 'acceptance', 'of', 'its', 'affiliated', 'fiat', 'money', 'within', 'a', 'certain', 'realm', '.']\nSentiment: Negative\n\n['Therefore', ',', 'in', 'order', 'to', 'serve', 'as', 'a', 'medium', 'of', 'exchange', ',', 'Bitcoin:LOCATION', 'has', 'to', 'rely', 'solely', 'on', 'the', 'self-fulfilling', 'expectation', 'on', 'the', 'part', 'of', 'private', 'agents', 'that', 'it', 'will', 'be', 'accepted', '.', \"''\"]\nSentiment: Negative\n\n['Claasen:PERSON', 'pointed', 'out', 'that', 'respondents', 'were', 'encouraged', 'to', 'participate', 'via', 'membership', 'email', 'list', ',', 'Facebook:ORGANIZATION', 'and', 'Twitter:ORGANIZATION', ',', 'Reddit:LOCATION', 'r/Bitcoin', 'and', 'r/BTC', '/', '.']\nSentiment: Negative\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'We', \"'re\", 'not', 'under', 'any', 'illusion', 'that', 'we', \"'re\", 'going', 'to', 'directly', 'influence', 'the', 'protocol', 'product', 'roadmap', 'during', 'this', 'plan', 'to', 'end:DATE', '2017:DATE', '.']\nSentiment: Negative\n\n['That', 'said', ',', 'we', 'have', 'community', 'support', 'for', 'getting', 'involved', 'in', 'key', 'areas', 'that', 'are', 'either', 'currently:DATE', 'unserved', 'or', 'underserved', '.']\nSentiment: Neutral\n\n['We', 'have', 'no', 'desire', 'to', 'compete', 'with', 'anyone', 'else', 'in', 'the', 'Bitcoin', 'community', '.']\nSentiment: Negative\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'It', \"'s\", 'clear', 'to', 'me', 'that', 'the', 'foundation', 'should', 'focus', 'its', 'advocacy', 'programs', 'on', 'Bitcoin', 'as', 'a', 'store', 'of', 'value', 'and', 'medium', 'of', 'exchange', 'during', '2016/2017:NUMBER', '.']\nSentiment: Negative\n\n['People', 'are', 'looking', 'for', 'another', 'option', 'in', 'key', 'areas', 'like', 'international', 'remittances', ',', 'micro', '-', 'and', 'peer-to-peer', 'payments', 'and', 'a', 'fiat', 'currency', 'hedge', 'and', 'they', 'need', 'to', 'know', 'more', 'about', 'Bitcoin:PERSON', 'in', 'this', 'context', '.', \"''\"]\nSentiment: Negative\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'said', 'the', 'survey', 'was', 'only', 'one:NUMBER', 'of', 'the', 'tools', 'that', 'was', 'used', 'in', 'formulating', 'the', 'plan', 'as', 'he', 'had', 'many', 'face-to-face', 'and', 'telephonic', 'conversations', 'with', 'key', 'players', 'in', 'the', 'community', 'over', 'the', 'last', 'while', 'that', 'have', 'also', 'shaped', 'our', 'plans', '.']\nSentiment: Negative\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'survey', 'shows', 'that', '41:PERCENT', '%:PERCENT', 'of', 'respondents', 'want', 'the', 'Bitcoin:ORGANIZATION', 'Foundation:ORGANIZATION', 'to', 'structure', 'its', 'operations', 'globally', 'by', 'having', 'initiatives', 'centrally', 'initiated', 'but', 'locally', 'managed', '.']\nSentiment: Negative\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['35:PERCENT', '%:PERCENT', 'want', 'them', 'to', 'be', 'either', 'locally', 'initiated', 'and', 'locally', 'managed', 'through', 'affiliated', 'chapters', ',', 'and', '15:PERCENT', '%:PERCENT', '-', 'centrally', 'initiated', 'and', 'centrally', 'managed', 'by', 'the', 'Foundation:ORGANIZATION', '.']\nSentiment: Negative\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'Philosophically', ',', 'the', 'community', 'does', 'not', 'wish', 'for', 'the', 'foundation', 'to', 'attempt', 'to', 'centralize', 'control', 'of', 'any', 'functions', 'and', 'we', \"'re\", 'very', 'supportive', 'of', 'decentralized', 'decision-making', 'by', 'the', 'community', 'in', 'the', 'context', 'of', 'co-ordinated', 'activity', '.']\nSentiment: Negative\n\n['There', 'is', 'much', 'work', 'to', 'be', 'done', 'in', 'this', 'area', 'because', 'it', \"'s\", 'so', 'new', 'to', 'everyone', 'in', 'the', 'community', '.', \"''\"]\nSentiment: Positive\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'another', 'area', 'touched', 'in', 'the', 'survey', ',', '42:PERCENT', '%:PERCENT', 'of', 'the', 'respondents', 'say', 'their', 'companies', 'are', 'not', 'currently:DATE', 'using', 'Bitcoin:LOCATION', 'in', 'any', 'way', 'and', '28:PERCENT', '%:PERCENT', 'having', 'companies', 'that', 'use', 'Bitcoin:PERSON', 'as', 'a', 'primary', 'product', 'or', 'service', 'and', '19:PERCENT', '%:PERCENT', 'currently:DATE', 'researching', 'using', 'Bitcoin', 'in', 'a', 'product', 'or', 'service', '.']\nSentiment: Negative\n\n['11:PERCENT', '%:PERCENT', 'have', 'a', 'company', \"'s\", 'products', 'use', 'Bitcoin', '.']\nSentiment: Negative\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['62:PERCENT', '%:PERCENT', 'of', 'respondents', 'would', 'be', 'willing', 'to', 'contribute', 'to', 'funding', 'Bitcoin:ORGANIZATION', 'Foundation:ORGANIZATION', 'operations', 'in', 'their', 'personal', 'capacity', 'in', 'return', 'for', 'a', 'published', 'quarterly:SET', 'plan', 'provided', 'they', 'can', 'see', 'its', 'plan', '.']\nSentiment: Negative\n\n['19:PERCENT', '%:PERCENT', 'each', 'both', 'agree', 'and', 'disagree', 'to', 'make', 'such', 'contributions', '.']\nSentiment: Negative\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On', 'how', 'the', 'Foundation', 'should', 'fund', 'its', 'activities', ',', 'some', 'respondents', 'say', 'it', 'should', 'be', 'primarily', 'through', 'individual/consumer', 'memberships', '-LRB-', '27:PERCENT', '%:PERCENT', '-RRB-', ',', 'corporate', 'sponsorships', '-LRB-', '20:PERCENT', '%:PERCENT', '-RRB-', ',', 'conferences', '-LRB-', '18:PERCENT', '%:PERCENT', '-RRB-', ',', 'business', 'memberships', '-LRB-', '14:PERCENT', '%:PERCENT', '-RRB-', ',', 'training', 'programs', '-LRB-', '12:PERCENT', '%:PERCENT', '-RRB-', 'and', 'other', 'means', '-LRB-', '10:PERCENT', '%:PERCENT', '-RRB-', '.']\nSentiment: Negative\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Claasen:PERSON', 'noted', 'that', 'from', 'an', 'organizational', 'structure', 'perspective', ',', 'an', 'army', 'of', 'volunteers', 'can', 'not', 'be', 'relied', 'upon', 'to', 'run', 'the', 'programs', 'of', 'the', 'Foundation', 'because', 'people', '``', 'do', \"n't\", 'have', 'large', 'blocks', 'of', 'time', 'to', 'commit', 'for', 'free', 'and', 'this', 'is', 'perfectly', 'reasonable', 'and', 'not', 'in', 'any', 'way', 'suggestive', 'of', 'a', 'lack', 'of', 'support', 'for', 'the', 'foundation', '.']\nSentiment: Negative\n\n['We', 'must', 'pay', 'for', 'people', \"'s\", 'time', ',', 'which', 'means', 'that', 'we', 'must', 'have', 'a', 'compelling', 'and', 'sustainable', 'revenue', 'model', 'that', 'does', \"n't\", 'create', 'conflicts', 'of', 'interest', '.', \"''\"]\nSentiment: Negative\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'added', 'that', 'they', 'are', 'working', 'with', 'the', 'Board:ORGANIZATION', 'and', 'key', 'stakeholders', 'on', 'a', 'comprehensive', 'operating', 'plan', '.']\nSentiment: Positive\n\n['``', 'Everything', 'that', 'we', \"'re\", 'going', 'to', 'be', 'doing', 'going', 'forward', 'will', 'be', 'driven', 'by', 'where', 'our', 'members', 'wants', 'us', 'to', 'focus', 'our', 'resources', '.']\nSentiment: Negative\n\n['We', 'will', 'be', 'completely', 'transparent', 'with', 'our', 'operating', 'and', 'financial', 'plan', '.']\nSentiment: Negative\n\n['I', 'ca', \"n't\", 'say', 'much', 'more', 'than', 'that', 'just', 'yet', 'beyond', 'commentary', 'on', 'the', 'survey', 'results', ',', 'but', 'will', 'do', 'so', 'soon', ',', \"''\", 'he', 'said', '.']\nSentiment: Neutral\n\n['For', 'updates', 'and', 'exclusive', 'offers', ',', 'enter', 'your', 'e-mail', 'below', '.']\nSentiment: Negative\n\n"
     ]
    }
   ],
   "source": [
    "# http://stanfordnlp.github.io/CoreNLP/corenlp-server.html#getting-started\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "def corenlp_analysis_all(in_file):\n",
    "    with open(in_file, 'r', encoding='utf8') as f:\n",
    "        content = \"\".join(f.readlines())\n",
    "\n",
    "    nlp = StanfordCoreNLP('http://127.0.0.1:9000')\n",
    "    output = nlp.annotate(\n",
    "        content,\n",
    "        properties={\n",
    "            'annotators': 'sentiment',\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for i, _ in enumerate(output['sentences']):\n",
    "        print(\"Sentence:\", [t['word'] for t in output['sentences'][i]['tokens']])\n",
    "        print(\"Sentiment:\", output['sentences'][i]['sentiment'])\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "def corenlp_analysis(in_file):\n",
    "    with open(in_file, 'r', encoding='utf8') as f:\n",
    "        # content = f.readlines()\n",
    "        lines = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "    nlp = StanfordCoreNLP('http://127.0.0.1:9000')\n",
    "    for line in lines:\n",
    "        output = nlp.annotate(\n",
    "            line,\n",
    "            properties={\n",
    "                'annotators': 'sentiment,ner',\n",
    "                'outputFormat': 'json'\n",
    "            }\n",
    "        )\n",
    "        # http://stanfordnlp.github.io/CoreNLP/ner.html\n",
    "        ner_names = [\"PERSON\", \"LOCATION\", \"ORGANIZATION\", \"MISC\"]\n",
    "        ner_numerical = [\"MONEY\", \"NUMBER\", \"ORDINAL\", \"PERCENT\"]\n",
    "        ner_temporal = [\"DATE\", \"TIME\", \"DURATION\", \"SET\"]\n",
    "        for i, _ in enumerate(output['sentences']):\n",
    "            print([t['word'] + (\":\" + t[\"ner\"] if t[\"ner\"] != \"O\" else \"\") for t in output['sentences'][i]['tokens']])\n",
    "            print(\"Sentiment:\", output['sentences'][i]['sentiment'])\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "in_dir = 'links_contents_clean'\n",
    "files = [\n",
    "    '20161001-2fc38645b2f1075b-Community_Prefers_Bitcoin_As_Exchange_Medium_Over_Store_of_Value%2C.txt.clean.txt',\n",
    "    # '20161001-51c397b0cd8bdbc6-Why_Blockchain_Won%E2%80%99t_Disrupt_Banks_First_-_CoinDesk.txt.clean.txt',\n",
    "    # '20161001-adb0edbb66d3fb0d-Kim_Dotcom_Reiterates_His_Bitcoin_Price_Forecast%2C_%242000_in_2.txt.clean.txt',\n",
    "    # '20161002-2c276484c2156687-Bitcoin_Can_Buy_You_a_Biometric_Data_Skimmer_on_the.txt.clean.txt',\n",
    "    # '20161002-c2b624fa4e45d409-Croatian_Law_Enforcement_Completes_Another_Bitcoin-related_Darknet_Drug_Bust_-.txt.clean.txt',\n",
    "    # '20161003-1eca38f66bf45f56-MGT_Capital_Investments_Inc_%28NYSEMKT%3AMGT%29%3A_Opportunity_Through_Uncertainty_%7C_Insider.tx.clean.txtt'\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    in_file = os.path.join(in_dir, file)\n",
    "    corenlp_analysis(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 a1b2c3d4\n10 sd\n14 sd\n17 abc\n\"---w---w---w...w,w-w*w/w\"\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "# # p = re.compile(r'\\\\b+.*\\\\b+')\n",
    "p = re.compile(r'(?:\\w+\\b|\\b\\w+\\b|\\b\\w+)')\n",
    "for m in p.finditer(' a1b2c3d4 sd. sd;abc'):\n",
    "    print(m.start(), m.group())\n",
    "s = re.sub(r'\\b\\w+\\b', 'w', '---aaa---word---word...word,wprd-w*2016/2017')\n",
    "print('\\\"%s\\\"' % s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'bc', 'de', 'aa', 'aaa', 'word', 'word', 'word', 'wprd', 'w', '2016', '2017']\n"
     ]
    }
   ],
   "source": [
    "words = re.split(r'[^\\w]+', 'aa bc de--aa ---aaa---word---word...word,wprd-w*2016/2017')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('have' in english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usa ee.uu. write ie Timothy R McTaggart and Matthew R Silver of. my example DECIMAL MegaUpload DECIMAL and BitCache 1. Read more 2016...\n"
     ]
    }
   ],
   "source": [
    "line = \"u.s.a. ee.uu. write i.e. Timothy R. McTaggart and Matthew R. Silver of. my example .1 MegaUpload 2.0 and BitCache 1. Read more 2016...\"\n",
    "line = re.sub('\\d*[.,]\\d+', 'DECIMAL', line)\n",
    "\n",
    "# print(re.sub(r'\\b(([a-zA-Z])\\.)+', '\\g<2>', line))\n",
    "# print(re.sub(r'\\b(?:([a-zA-Z])\\.)+', '*', line))\n",
    "p = re.compile(r'\\b((?:[a-zA-Z]\\.)+)')\n",
    "for m in p.finditer(line):\n",
    "    line = line.replace(m.group(), ''.join(m.group(1).split('.')))\n",
    "    # print(m.start(), m.group(), ''.join(m.group(1).split('.')))\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it usa ee', 'uu', ' write ie Timothy R McTaggart and Matthew', ' Inc (NYSEMKT:MGT)', ' my example MegaUpload 2', '0 and BitCache', ' Read more', ' \"hello worl', '\"']\n['it usa ee', ' write ie Timothy R McTaggart and Matthew', ' my example MegaUpload 2', '0 and BitCache']\n"
     ]
    }
   ],
   "source": [
    "line = 'it usa ee.uu. write ie Timothy R McTaggart and Matthew. Inc (NYSEMKT:MGT). my example MegaUpload 2.0 and BitCache. Read more... \"hello worl.\"'\n",
    "#FAILS WITH: MGT Capital intends to change its corporate name to “John McAfee Global Technologies, Inc.”\n",
    "print([s for s in re.split(r'\\.', line) if s])\n",
    "print([s for s in re.split(r'\\.+', line) if len(s.split()) > 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}